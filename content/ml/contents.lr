_model: ml-curriculum
---
_template: ml.html
---
_discoverable: no
---
curriculum:

## How to Learn Machine Learning

Having the goal of "learning machine learning" is daunting. I've found the best way to make it tractable is to approach it in phases. Each phase should include at least one track that builds practical skills and one track focused on theoretical foundations. Additionally, it's always worth surveying the field at your current level of fluency to be on the look out for the next phase of studies and to continue to build a mental map of interconnected topics that may be prerequisites for the techniques and applications that you find most exciting.

Each track should focus on a specific curriculum resource, and then draw on supporting resources. For instance, you might choose a specific book or MOOC course you want to work through, and then draw on several related resources to cross-reference as you proceed. Sticking with a single main resource is important to staying focused; it's really easy and tempting to jump around various resources without making as much progress past introductory material. It's worth spending time upfront researching curriculum options before deciding; I usually find several good resources on a topic and do some initial skimming before deciding which becomes the primary resource, bookmarking the rest as supporting.

### Phase 1: Applied Machine Learning & Probability & Statistics

Phase 1, which took me about 5 months to complete full-time studying, includes two tracks:

<img src="ml-curriculum-phase1.svg" width="100%">

#### Track 1: Probability and Statistics

The goal of this track is to get comfortable with basic statistics and exploratory data analysis, and to build a solid theoretical foundation in probability theory that will make thinking more rigorously about machine learning possible.  IMHO it is insufficient to rely solely on the brief intros to probability contained at the beginning of many machine learning texts; eating your spinach early will payoff over and over again as you approach more advanced models and techniques later on. As Wasserman puts it in the preface to All of Statistics:

> Using fancy tools like neural nets, boosting, and support vector machines without understanding basic statistics is like doing brain surgery before knowing how to use a band-aid.

Slowing way down and struggling through real math problems is key here. I'd often spend hours, sometimes days on a single problem. Getting stuck = real learning. I recommend finding a study buddy and/or expert who's willing to help you get over the hump when you are truly stuck.

The track:

1. [Stanford's Free online Probability and Statistics Course](https://lagunita.stanford.edu/courses/OLI/ProbStat/Open/about): a nice gentle introduction before approaching the more axiomatic coverage of Wasserman. Good coverage of basic exploratory data analysis too.
2. The first 5 chapters of Wasserman's [All of Statistics](http://www.amazon.com/All-Statistics-Statistical-Inference-Springer/dp/0387402721) text and problem sets from CMU's [intermediate stats course (from the author)](http://www.stat.cmu.edu/~larry/=stat705/) and [another more introductory counterpart](https://web.archive.org/web/20151110005906/http://www.stat.cmu.edu/~jinglei/Fall15.shtml)

Supporting resources:

- [Khan Academy's videos on Probability and Statistics](https://www.khanacademy.org/math/probability?t=table-of-contents): I didn't watch comprehensively, but did turn to for a second explanation many times.
- [Math Monk's YouTube playlist: Probability Primer](https://www.youtube.com/playlist?list=PL17567A1A3F5DB5E4): These felt like the missing MOOC lectures for Wasserman's book
- [UAH's Random: Probability, Mathematical Statistics, Stochastic Processes](http://www.math.uah.edu/stat/): comprehensive coverage of many important topics in probability and statistics, including some illustrative web app / simulations and exercises. 
- [First Look at Rigorous Probability Theory](https://www.amazon.com/First-Look-Rigorous-Probability-Theory/dp/9812703713/ref=cm_cr_arp_d_product_top?ie=UTF8): another good textbook that I bought and consulted for a second explanation on many of the topics covered in Wasserman
- [Peter Norvig's Introduction to Probability iPython Notebook](http://nbviewer.jupyter.org/url/norvig.com/ipython/Probability.ipynb)
- Wikipedia's outlines of [Statistics](https://en.wikipedia.org/wiki/Outline_of_statistics) and [Probability](https://en.wikipedia.org/wiki/Outline_of_probability)
- [IUPUI's ECE 302 Probabilistic Methods in Electrical Engineering](http://www.engr.iupui.edu/~skoskie/ECE302/ECE302_s15.html): course website with nicely written up homework and exam solutions.
- [Penn State's Stat 414/415 Course Materials](https://onlinecourses.science.psu.edu/stat414/): another good place to cross reference concepts with some examples and solutions. I found perusing the [section on functions of random variables](https://onlinecourses.science.psu.edu/stat414/node/127) helpful and wish I'd found it sooner!
- [Guy Lebanon's The Analysis of Data Vo1: Probability](http://theanalysisofdata.com/probability/): Prof turned industry ML champ (LinkedIn, Netflix) published a free book on probability theory. A nice resource to cross reference concepts covered in the 1st half of Wasserman. Guy also has a bunch of [notes](http://www.cc.gatech.edu/~lebanon/notes/) on his website that are interesting to peruse.
- [Seeing Theory](http://students.brown.edu/seeing-theory/): very nice visualizations to aid understanding of fundamental concepts in probability and statistics

What I appreciate about the All of Statistics book compared to others I've looked at, including my text from college, is that it doesn't spend too much time on counting methods (knowing how many ways one can deal a full house with a deck of cards isn't particularly relevant) and is otherwise more comprehensive on probability theory most relevant to machine learning. It is concise and somewhat dry, but it serves as a great road map of topics to study; the supporting resources and lectures can provide the additional context when necessary. Math Monk's videos are a particularly nice companion.

#### Track 2: Applied Machine Learning

The goal of this track is to gain practical experience applying supervised and unsupervised learning and data analysis techniques using Python, [Scikit-learn](http://scikit-learn.org/) and [Jupyter notebooks](http://jupyter.org/) and many of the practical considerations wrangling data using tools like [Pandas](http://pandas.pydata.org/) and [Numpy](http://www.numpy.org). By the end you will be able to build and evaluate predictive models that work with real data, and get exposed to many theoretical models. It's nice to get your feet wet and gain powerful skills right away even if you don't yet fully understand how everything works under the hood.

The track:
- [Python Machine Learning](http://www.amazon.com/Python-Machine-Learning-Sebastian-Raschka/dp/1783555130) and accompanying notebooks [available on github](https://github.com/rasbt/python-machine-learning-book)
- [Kaggle](http://kaggle.com) competitions starting with [two](https://www.kaggle.com/c/titanic) [basic](https://www.kaggle.com/c/forest-cover-type-prediction) classification challenges (as recommended [here](https://www.quora.com/What-Kaggle-competitions-should-a-beginner-start-with-1)) and moving onto whatever catches your eye.

Supporting resources:
- Andrew NG's [Machine Learning coursee](https://www.coursera.org/learn/machine-learning/): even if not worked through as a main track, worth perusing video lectures for any overlapping topics, NG does a great job of explaining things.
- [scikit-learn website](http://scikit-learn.org/stable/)

The Python Machine Learning book provides a great blend of practical concerns working with data (preprocessing, cross-validating) and exposure to models used for classification, regression and unsupervised learning and even gets into ensemble methods. By the end of chapter 4 you should be ready to take on your first Kaggle competition, and by the time you finish, approach many more and even tackle interesting new analysis problems that interest you. I recommend reading the book and working through all of the examples in a Jupyter notebook, looking at his provided notebooks whenever necessary and/or to copy over boilerplate code.

#### Survey the field

These are the resources that inspired me to leap into machine learning head first, and continued to provide companionship throughout phase 1.

- [Talking Machines Podcast](http://www.thetalkingmachines.com): each episode includes an introduction to a topic and an interview with an expert in the field. 
- [The Master Algorithm](http://www.amazon.com/The-Master-Algorithm-Ultimate-Learning/dp/0465065708): great lay persons overview of the field. Also see [my review on Amazon](http://www.amazon.com/review/R1BR5BVSKJAE7A).
- [Becoming a data scientist podcast](http://www.becomingadatascientist.com): interviews with people who've successfully pursued or transitioned to careers in data science, which has plenty of overlap with aspirations to apply machine learning.

I also bought [Bishop's PRML](https://www.amazon.com/gp/product/0387310738) and [Murphy's MLPP](https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020) a couple of months into my studies and wish I had sooner. They are great for perusing to plant seeds for future studies, and to begin connecting the dots from exciting advanced concepts back to probability theory.

### Phase 2: Probabilistic ML and Computer Vision

The goal of phase 2 is to build on the theoretical knowledge of probability theory from phase 1 to gain a richer, probabilistic understanding of machine learning, and build on the practical skills by diving into a more advanced topic.

<img src="ml-curriculum-phase2.svg" width="100%">

I think track 1 would be appropriate for everyone, and track 2 depends on what field of machine learning you are most interested in (and perhaps where you have taken a job!); in my case it is computer vision, but could just as well be something like natural language processing, or bioinformatics.

#### Track 1: Probabilistic ML

The track:
- Read and work through [Bishop's Pattern Recognition and Machine Learning  book](https://www.amazon.com/gp/product/0387310738)
- Author [notebooks](/ml/notebooks) about topics along the way (e.g [Expectation Maximization](/ml/notebooks/em-coin-flips/)

Supporting Resources:
- [Kevin Murphy's Machine Learning from a Probabilistic Perspective](https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020): was a close second for the learning track. More comprehensive than PRML, but feels more like a great reference than a book to work through cover to cover.
- [Coursera course on Probabilistic Graphical Models](https://www.coursera.org/learn/probabilistic-graphical-models): the original course has been broken into 3. I've found the video lectures a great complement to the coverage of graphical models in Bishop's book.
- Notebook Lectures from University of Michigan's [EECS 545](https://github.com/thejakeyboy/umich-eecs545-lectures) and [EECS 445](https://github.com/eecs445-f16/umich-eecs445-f16) Machine Learning courses (some of which [I helped to develop!](/ml/learning-log/2016-12-22/))
- [Mathematical Monk's ML YouTube playlist](https://www.youtube.com/playlist?list=PLD0F06AA0D2E8FFBA)

This track is all about going deeper into the theory underlying machine learning, often viewing models in terms of joint probability distributions. Why bother? Well, beyond viewing machine learning fields like supervised learning as a useful black box that can make predictions, being able to reason more soundly about how confident you are in the model's predictions requires it. And as you wade into more advanced topics and bayesian methods, you will find you simply cannot understand the material without fluently seeing how things are modeled probabilistically, and reasoning about when and how you can infer the model from data, for instance, which models provide for exact inference, and which require sampling methods like MCMC.

Having spent time perusing both MLPP and PRML during phase 1 was helpful in determining which book to choose. I ultimately decided that MLPP was a better choice as I find it does a more thorough job covering the fundamentals and structuring the book to progress linearly. PRML both benefits and is burdened with nearly a decade more material, so it feels more like a really good and pretty thorough survey of nearly every field of ML. That it is nearly a thousand pages long also means it would be pretty impractical to attempt to read it cover to cover in a single 4-6 month phase. And while MLPP is "out of date", everything in it is feels like essential material and should be covered before moving onto the more recent material covered in PRML.

#### Track 2: Computer Vision

The track:

- Stanford's cs231n course [materials](http://cs231n.github.io/) and
[video lectures](https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC)

Supporting resources:

- [Hugo Larochelle's deep learning lectures](https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH): could be a learning track in itself. Covers conv nets, great for cross referencing.
- [Understanding Higher Order Local Gradient Computation for Backpropagation in Deep Neural Networks](https://danieltakeshi.github.io/2017/01/21/understanding-higher-order-local-gradient-computation-for-backpropagation-in-deep-neural-networks/): nice tips on reasoning about computing gradients of functions of tensors with respect to tensors from Daniel Seita, who TA'd the cs231n-like course at Berkeley
- [Vector Deriviatives notes](http://cs231n.stanford.edu/vecDerivs.pdf): more notes on computing derivatives of tensors from Erik Learned-Miller

Once I started [a job](http://fcav.engin.umich.edu) helping with research related to autonomous vehicles, the most exciting practical application of ML became computer vision. Examples of core tasks include image classification (given an image, what is it), object detection (given an image, where are the things in it, and what are they) and pose detection (given this image of a person, how are they oriented). I've had a chance to learn a lot about a lot of topics, but a lot of focus at the state of the art involves various applications of deep convolutional neural networks. So I'm focusing on learning the fundamentals of convolutional neural networks instead of some of the more fundamental topics within computer vision like multi-view geometry.

Stanford's cs231n course is perfect for mastering convolutional neural networks as it presents the theory and has assignments that require implementing the core required models. The main instructor, Andrej Karpathy, is a great teacher too. I have a github repo for my WIP solutions [here](https://github.com/krosaen/cs231n).
 
#### Survey the field: read books and papers!

I'm finding that during this phase I'm capable of reading recent research (e.g the papers on object detection [listed here](https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap)) and watching research talks (e.g [this one about scaleable gaussian processes](http://slideshot.epfl.ch/play/k5FuJcUA0L0c). It feels similar to how perusing PRML and MLPP felt during phase 1: I don't always understand everything, but I continue to build a mental map of what lies ahead and get a sense of what is most exciting.

## Future Phases and tracks

I won't know for sure until I've completed phase 2, but I think future (and life long!) studying of machine learning will likely consist of diving deep into particular topics in machine learning, mathematics, computer science and engineering. Some ideas: generative adversarial networks, reinforcement learning, real analysis, information theory, projective geometry, and high performance numerical computing. I will update this section as ideas for future tracks become clear.

## Prior Art

I'm far from the first person to give advice on studying machine learning; here are some resources I've found helpful along the way:

- [The Open Source Data Science Masters](http://datasciencemasters.org): another person who laid out a curriculum and worked through it.
- [Quora thread: How do I learn Machine Learning?](https://www.quora.com/Machine-Learning/How-do-I-learn-machine-learning-1)
- [Recommended ML Curriculum](http://sebastianraschka.com/faq/docs/ml-curriculum.html) from Sebastian Raschka, the author of the Python Machine Learning book that I'm working through
- [Metacademy Roadmaps](https://www.metacademy.org/roadmaps/): guides to learning ML and how to learn on your own in general
- [Dive into Machine Learning](https://github.com/hangtwenty/dive-into-machine-learning): a good guide if you wish to get your hands dirty ASAP
- [Metromap diagram to becoming a data scientist](http://nirvacana.com/thoughts/becoming-a-data-scientist/): cool way to visualize suggested curriculum across subfields
- [How to Start Learning Deep Learning](http://ofir.io/How-to-Start-Learning-Deep-Learning/): nice roundup of recent material on deep learning
- Xavier Amatriain's (VP of Eng at Quora) [How should you start a career in Machine Learning?](https://www.quora.com/How-should-you-start-a-career-in-Machine-Learning/answer/Xavier-Amatriain) and [How do I learn machine learning?](https://www.quora.com/How-do-I-learn-machine-learning-1/answer/Xavier-Amatriain)
- François Chollet (author of Keras) on [What advice would you give to people studying ML/DL from MOOCs (Udacity, Coursera, edx, MIT Opencourseware) or from books in their own time?](https://www.quora.com/What-advice-would-you-give-to-people-studying-ML-DL-from-MOOCs-Udacity-Coursera-edx-MIT-Opencourseware-or-from-books-in-their-own-time/answer/Fran%C3%A7ois-Chollet)
- [HOW TO LEARN ADVANCED MATHEMATICS WITHOUT HEADING TO UNIVERSITY - PART 1](https://www.quantstart.com/articles/How-to-Learn-Advanced-Mathematics-Without-Heading-to-University-Part-1): a London quant's curriculum list for advanced mathematics. Great list of resources for linear algebra, real analysis, foundations of mathematics and the like.
- [Machine Learning for Software Engineers](https://github.com/ZuzooVn/machine-learning-for-software-engineers): very thoroughly researched curriculum and study plans


---
intro:

I recently [embarked on a learning sabbatical](/learning-sabbatical) focused on machine learning. Here are resources for you to follow along if you like.


---
resoures:

## Full list of resources

My curriculum includes 3 areas:

- Probability and Statistics fundamentals: [All of Statistics](http://www.amazon.com/All-Statistics-Statistical-Inference-Springer/dp/0387402721) text and problem sets from [CMU's intermediate stats course (from the author)](http://www.stat.cmu.edu/~larry/=stat705/), cross referencing with [Mathematical Monk's probability primer playlist](https://www.youtube.com/playlist?list=PL17567A1A3F5DB5E4), [Stanford's Free online Probability and Statistics Course](https://lagunita.stanford.edu/courses/OLI/ProbStat/Open/about), and [Khan Academy's videos on Probability and Statistics](https://www.khanacademy.org/math/probability?t=table-of-contents).
- Applied ML techniques:  [Python Machine Learning](http://www.amazon.com/Python-Machine-Learning-Sebastian-Raschka/dp/1783555130)
- Some starter [Kaggle competitions](https://www.quora.com/What-Kaggle-competitions-should-a-beginner-start-with-1) and a Capstone project

I view this as attacking understanding and ability in machine learning from the bottom, the middle and the top. I'm trying to avoid the temptation of putzing around the latest and greatest tutorials and libraries for things like TensorFlow until I get a little bit further with the fundamentals, but I look forward to that eventually; it will be my dessert after eating lot's of vegetables.

#### Probability and Statistics

Much of machine learning can be viewed as statistical inference. I get the sense that the machine learning community is a bit more willing to use whatever tools empirically work than the applied stats community, which prefers to have stronger theoretical underpinnings for how/why a particular statistical model can be predictive under certain circumstances (the [Talking Machines Podcast](http://www.thetalkingmachines.com) as well as [The Master Algorithm](http://www.amazon.com/The-Master-Algorithm-Ultimate-Learning/dp/0465065708) book are both great for getting this overview). But the two fields are very tightly related and the more I read the more it seems like it's often a tomato / tomahto situation, for instance, [this paper](http://www.robots.ox.ac.uk/~szheng/papers/CRFasRNN.pdf) showing how conditional random fields, a probabilistic graphical model (the fanciest kind of statistical model as far as I can tell) can be modeled as a recurrent neural network. The [section on graphical models in The Deep Learning book](http://www.deeplearningbook.org/contents/graphical_models.html) also gives a nod to the relationship between the two fields.

With this in mind, if I want to have a deeper intuition for how many of these techniques work, I really need to understand probability and statistics deeply. And it's not easy to just dive right into conditional random fields if you don't know the basics about random variables, probability distributions and the like cold. I think it's also important to understand bread and butter techniques like linear and logistic regression as they should be the first thing attempted on most datasets before fancier models are deemed necessary.

So my goal in this section is to start at the bottom and work my way through:

- [All of Statistics](http://www.amazon.com/All-Statistics-Statistical-Inference-Springer/dp/0387402721) book and problem sets from CMU's [course from the author of the](http://www.stat.cmu.edu/~larry/=stat705/) and its more introductory CS counterpart CS 36-700.
  - [CS 36-705 course website](http://www.stat.cmu.edu/~larry/=stat705/) with some problem sets and solutions available
  - [CS 36-700](https://web.archive.org/web/20151110005906/http://www.stat.cmu.edu/~jinglei/Fall15.shtml) HW Files and solutions (web archive page, links to hw and solution pdfs on dropbox still work as of 8/2016)
- [Mathematical Monk's Probability Primer](https://www.youtube.com/playlist?list=PL17567A1A3F5DB5E4): excellent video series with the same level of mathematical rigor as the All of Statistics book.
- [Stanford's Free online Probability and Statistics Course](https://lagunita.stanford.edu/courses/OLI/ProbStat/Open/about): a very basic introduction to statistics; it doesn't get to random variables until about half way through. While has seemed almost *too* basic at times, for instance reviewing the mean, median and mode, I'm already glad to have spent time on it and have spiced it up by applying some of the basics to data sets of interest to me (e.g [box plots of NBA teams' margin of victory](http://nbviewer.jupyter.org/github/krosaen/ml-study/blob/master/basic-stats/nba-games-net-rating-boxplots/NbaTeamGameNetRatingsPlots.ipynb)).
- [Khan Academy's videos and exercises on Probability and Statistics](https://www.khanacademy.org/math/probability?t=table-of-contents): really good material for the introductory stuff covered by Stanford's course. A good place to look for a second explanation, though I'm not working through these exhaustively.

What I appreciate about the All of Statistics book compared to others I've looked at, including my text from college, is that it doesn't spend too much time on counting methods (knowing how many ways one can deal a full house with a deck of cards isn't particularly relevant) and is otherwise more comprehensive, getting into some techniques for statistical inference.

#### Applied ML Techniques

The goal of this area is to have confidence actually applying machine learning techniques to real data sets and includes many practical considerations such as cleaning up and normalizing data, choosing the right model, cross validating to avoid overfitting, evaluating performance and things like that. I already worked through about half of Andrew NG's course (now [on Coursera](https://www.coursera.org/learn/machine-learning/)) years ago, and am tempted to start over, but I recently came across a book, [Python Machine Learning](http://www.amazon.com/Python-Machine-Learning-Sebastian-Raschka/dp/1783555130) that is just fantastic. Similar to NG's course, it mixes theoretical background, hand implemented algorithms and use of off the shelf tools, and I prefer to work through examples in Python and IPython notebooks, and it will be nice to get familiar with [Scikit-learn](http://scikit-learn.org/). It's also great that the author has made the source code for all notebooks related to the book [available on github](https://github.com/rasbt/python-machine-learning-book).

So far [I've begun worked through the book example by example](http://nbviewer.jupyter.org/github/krosaen/ml-study/blob/master/python-ml-book/) myself, avoiding looking at his solutions if at all possible.

#### Capstone Project

Once I'm about halfway through the ML book, I want to start carving out time to regularly work on a capstone project: something that will challenge me to apply some of the skills I've gained and also inspire further study of topics that become important to solving the challenge at hand. I have tons of ideas and will share more when things become concrete.

One great resource for this is [Kaggle](https://www.kaggle.com) which hosts data science competitions. There are many real world data sets and problems, and you are welcome to submit a model and see how you fare even after a competition closes. Winners of each competition often post their solutions. I view entering a Kaggle competition or two as a good fallback should my ideas prove unrealistic. Here are some resources for finding good starter kaggle projects:

- [Quora: What Kaggle competitions should a beginner start with?](https://www.quora.com/What-Kaggle-competitions-should-a-beginner-start-with-1)
- [Quora: https://www.quora.com/What-background-do-I-need-to-do-Kaggle-competitions-1](What background do I need to do Kaggle competitions?)
- [Kaggle Tutorials](https://www.kaggle.com/wiki/Tutorials)

*Update*: I've completed a couple of kaggle competitions, you can find write ups on the [notebooks section](/ml/notebooks/). I've also begun working on a side project concerning automatic data science. Read my [learning log](/ml/learning-log) to keep up with progress.



### Exhaustive list of Resources

Here are the resources on my radar—let me know if there's anything not here that I should seriously consider or that might bump one of the choices made above.

Some areas, like linear algebra, I've decided to skip completely for the time being and get by on my rusty skills as long as I can, but it may turn out I find it becomes vital to brush up.

### ML




- [Andrew NG's ML course](https://www.coursera.org/learn/machine-learning)
- [The Elements of 
Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/)
- [Machine Learning: A Probabilistic Perspective](https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020) Kevin Murphy's classic. Extremely comprehensive. Daunting. I found this more interesting to start browsing once I'd spent a couple months studying 'All of Statistics' and wanted to put various algorithms and approaches in a larger perspective.
- [Advanced Data Analysis from an Elementary Point of View](http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/)
- [Machine Learning for Hackers](http://xyclade.github.io/MachineLearning/)
- [The Deep Learning Book](http://www.deeplearningbook.org/contents/graphical_models.html)
  - this has a great [overview of graphical models](http://www.deeplearningbook.org/contents/graphical_models.html)
- [Data Science from Scratch](http://www.amazon.com/Data-Science-Scratch-Principles-Python/dp/149190142X): I bought this book and have enjoyed perusing.
- [Mathematical Monk's ML YouTube playlist](https://www.youtube.com/playlist?list=PLD0F06AA0D2E8FFBA)
- [Coursera course on Probabilistic Graphical Models](https://class.coursera.org/pgm/lecture/preview): this has been annoyingly pulled down pending a re-launch. I've heard that the original lecture and assignment materials are available as a torrent.
- [Probabilistic Programming & Bayesian Methods for Hackers](http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/)
- [Text on Probabilistic Graphical Models for sale](http://www.amazon.com/Probabilistic-Graphical-Models-Principles-Computation/dp/0262013193) (and [for download](https://github.com/JimmyLin192/GraphicalModel/blob/master/Probabilistic%20Graphical%20Models%20Principles%20and%20Techniques.pdf))
- [Tom Griffiths' reading list on Bayesian methods](http://cocosci.berkeley.edu/tom/bayes.html)
- [Doing Bayesian Data Analysis](http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/) and examples ported to [Python and PyMC3](https://github.com/aloctavodia/Doing_bayesian_data_analysis) and [iPython notebook](http://nbviewer.jupyter.org/github/aloctavodia/Doing_bayesian_data_analysis/blob/master/IPython/Kruschkes_Doing_Bayesian_Data_Analysis_in_PyMC3.ipynb)
- [Notebook Lectures from University of Michigan EECS 545: Machine Learning](https://github.com/thejakeyboy/umich-eecs545-lectures)
- [Computer Vision:  Models, Learning, and Inference](http://www.computervisionmodels.com/) Full text available as PDF. Can get nice hard copy from [Amazon](https://www.amazon.com/Computer-Vision-Models-Learning-Inference/dp/1107011795/ref=sr_1_1?s=books&ie=UTF8&qid=1334662414&sr=1-1) too.

### Probability and statistics

- [All of Statistics - A Concise Course in Statistical Inference](http://www.stat.cmu.edu/~larry/all-of-statistics/)
- [CMU's Intermediate Statistics course with problem sets / solutions from book](http://www.stat.cmu.edu/~larry/=stat705/)
- [Math Monk's YouTube playlist: Probability Primer](https://www.youtube.com/playlist?list=PL17567A1A3F5DB5E4)
- [On Probability Axioms and Sigma Algebras](http://ee.usc.edu/stochastic-nets/docs/probability-axioms-sigma-algebras.pdf): a nice complement to all of stats and Math Monk videos for grokking when measure theory is important to probabiltiy axioms
- [Khan Academy's videos and exercises on Probability and Statistics](https://www.khanacademy.org/math/probability?t=table-of-contents)
- [Stanford's online intro to Probability and Statistics](https://lagunita.stanford.edu/courses/OLI/ProbStat/Open/about)
-  [The Theory of Probability](https://www.amazon.com/Theory-Probability-Explorations-Applications/dp/1107024471): another text book that takes more time to motivate the need for the theory of probability while providing the same rigor as All of Statistics. It covers less material and is a bit longer though, revealing a tradeoff between providing background vs concision and breadth. I've found this to be a great second resource and would have considered working my way through it as a main resource had I found it earlier.
- [Guy Lebanon's The Analysis of Data Vo1: Probability](http://theanalysisofdata.com/probability/): Prof turned industry ML champ (LinkedIn, Netflix) published a free book on probability theory. A nice resource to cross reference concepts covered in the 1st half of Wasserman. Guy also has a bunch of [notes](http://www.cc.gatech.edu/~lebanon/notes/) on his website that are interesting to peruse.
- [Penn State's Stat 414/415 Course Materials](https://onlinecourses.science.psu.edu/stat414/): another good place to cross reference concepts with some examples and solutions. I recently found perusing the [section on functions of random variables](https://onlinecourses.science.psu.edu/stat414/node/127) helpful and wish I'd found it sooner!
- [Penn State's Stat 505 Course Materials](https://onlinecourses.science.psu.edu/stat505/): applied multivariate statistical analysis
- [UAH's Random: Probability, Mathematical Statistics, Stochastic Processes](http://www.math.uah.edu/stat/): comprehensive coverage of many important topics in probability and statistics, including some illustrative web app / simulations and exercises. Creative Commons license. Another great resource to cross reference.
- [IUPUI's ECE 302 Probabilistic Methods in Electrical Engineering](http://www.engr.iupui.edu/~skoskie/ECE302/ECE302_s15.html): course website with nicely written up homework and exam solutions.
- [Peter Norvig's Introduction to Probability iPython Notebook](http://nbviewer.jupyter.org/url/norvig.com/ipython/Probability.ipynb)
- [University of Michigan's SOCR probability & stats ebook](http://wiki.socr.umich.edu/index.php/EBook): worth cross referencing for comprehensive list of topics
- Wikipedia's outlines of [Statistics](https://en.wikipedia.org/wiki/Outline_of_statistics) and [Probability](https://en.wikipedia.org/wiki/Outline_of_probability)
- [Free Introduction to Probability Book from Dartmouth](http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/pdf.html)

### Linear Algebra

- [Linear Algebra Notes](http://www.cs.cmu.edu/~zkolter/course/linalg/linalg_notes.pdf)
- [Free Linear Algebra Text](http://joshua.smcvt.edu/linalg.html/)
- [Linear Algebra course MIT's Open Courseware](http://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/index.htm)

### Math & Proofs fundamentals

- [Introduction to Mathematical Thinking](https://www.amazon.com/gp/product/0615653634/ref=crt_ewc_title_dp_4?ie=UTF8&psc=1&smid=ATVPDKIKX0DER): the at-cost primer written to accompany [the coursera course](https://www.coursera.org/learn/mathematical-thinking): perfect motivation for studying mathematical thinking. The coursera course can be found as a torrent if you can't wait for the next enrollment.
- [Thinking Mathematically](https://www.amazon.com/gp/product/0273728911/ref=oh_aui_detailpage_o07_s00?ie=UTF8&psc=1): I ordered a used copy that took 2 weeks to ship from the UK, worth the wait (and ~\$20 instead of $100+ new). I found it to be a more approachable alternative to [Polya's How to Solve it](https://en.wikipedia.org/wiki/How_to_Solve_It)
- [Introduction to Proofs, an Inquiry-Based approach](http://joshua.smcvt.edu/proofs/)
- "Everything You Always Wanted To Know About Mathematics (But didn’t even know to ask)"—the text for [CMU's Concepts of Mathematics course](https://www.andrew.cmu.edu/course/21-127/) by [Brendan Sullivan](http://math.cmu.edu/~bwsulliv/): he apparently wrote this book for [his PHD thesis](http://math.cmu.edu/~bwsulliv/defense.pdf). The book doesn't seem to be available for purchase or download online, but the author was kind enough to send me a copy when I emailed and asked: how cool!
- [Jeremy Kun's "methods of proof" primers](https://jeremykun.com/primers/)
- [Introduction to higher mathematics playlist](https://www.youtube.com/playlist?list=PLZzHxk_TPOStgPtqRZ6KzmkUQBQ8TSWVX)
- [Introduction to mathematical arguments](https://math.berkeley.edu/~hutching/teach/proofs.pdf): background handout for courses requiring proofs from Michael Hutchings of Berkeley

I have a sense that if I were comfortable proving, or at least following the proofs of important theorems in statistics, deeper insights would stick. I think that's the problem I've had in the past with stats coursework: it's so easy to glaze over 3 pages of dense text that includes 4-5 theorems without understanding much more than how to plug and chug. One theorem I've already spent some time with proving and re-proving is Bayes', and many of the [problem sets](/ml/hw/) I've worked through have required some basic proofs.

While the idea that I will become a master mathematician along the way is quixotic, I think spending some time on mathematical thinking and problem solving is a nice prerequisite for the kind of reasoning required in thinking about machine learning probabilistically.

### Getting inspired

If you are just dabbling in ML here are some great resources to get you inspired and give you a lay of the land:

- [Talking Machines Podcast](http://www.thetalkingmachines.com)
- [The Master Algorithm](http://www.amazon.com/The-Master-Algorithm-Ultimate-Learning/dp/0465065708) and [my review on Amazon](http://www.amazon.com/review/R1BR5BVSKJAE7A)
- [Becoming a data scientist podcast](http://www.becomingadatascientist.com)

### Prior Art

This is not an original idea and there are some great posts & resources out there about a machine learning curriculum including:

- [The Open Source Data Science Masters](http://datasciencemasters.org): another person who laid out a curriculum and worked through it.
- [Quora thread: How do I learn Machine Learning?](https://www.quora.com/Machine-Learning/How-do-I-learn-machine-learning-1)
- [Recommended ML Curriculum](http://sebastianraschka.com/faq/docs/ml-curriculum.html) from Sebastian Raschka, the author of the Python Machine Learning book from Phase 1
- [Metacademy Roadmaps](https://www.metacademy.org/roadmaps/): guides to learning ML and how to learn on your own in general
- [Dive into Machine Learning](https://github.com/hangtwenty/dive-into-machine-learning): a good guide if you wish to get your hands dirty ASAP
- [Metromap diagram to becoming a data scientist](http://nirvacana.com/thoughts/becoming-a-data-scientist/): cool way to visualize suggested curriculum across subfields
- [How to Start Learning Deep Learning](http://ofir.io/How-to-Start-Learning-Deep-Learning/): nice roundup of recent material on deep learning
- Xavier Amatriain's (VP of Eng at Quora) [How should you start a career in Machine Learning?](https://www.quora.com/How-should-you-start-a-career-in-Machine-Learning/answer/Xavier-Amatriain) and [How do I learn machine learning?](https://www.quora.com/How-do-I-learn-machine-learning-1/answer/Xavier-Amatriain)
- François Chollet (author of Keras) on [What advice would you give to people studying ML/DL from MOOCs (Udacity, Coursera, edx, MIT Opencourseware) or from books in their own time?](https://www.quora.com/What-advice-would-you-give-to-people-studying-ML-DL-from-MOOCs-Udacity-Coursera-edx-MIT-Opencourseware-or-from-books-in-their-own-time/answer/Fran%C3%A7ois-Chollet)
- [How to Learn Advanced Mathematics Without Heading to University - Part 1](https://www.quantstart.com/articles/How-to-Learn-Advanced-Mathematics-Without-Heading-to-University-Part-1): a London quant's curriculum list for advanced mathematics. Great list of resources for linear algebra, real analysis, foundations of mathematics and the like.

### Starting Point

I'm not starting from scratch: I've written software professionally for many years in everything from C/C++ and Java to Python and Ruby and have a masters degree in computer engineering, which included a general engineering education with lot's of calculus courses. I've also dabbled with different ML tools and perused books over the years. So my curriculum doesn't include things like learning Python or how to slurp data from websites and clean things up and you may wish to consult other guides for good resources on the fastest way to get up to speed on programming as it most relates to ML / data science.

